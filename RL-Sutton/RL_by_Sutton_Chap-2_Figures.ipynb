{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:25<00:00, 79.17it/s]\n",
      " 64%|██████▎   | 1274/2000 [00:17<00:08, 80.73it/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, eps = 0.0, arm = 10, step_size = 0.1, init_value=0.0, UCB_val = None, sample_average = False,\\\n",
    "                 gradient=False, baseline=False, expected_reward=0.0):\n",
    "        self.arm = arm\n",
    "        self.initial_value = init_value\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = eps\n",
    "        self.time = 0\n",
    "        self.sample_average = sample_average\n",
    "        self.UCB_val = UCB_val\n",
    "        self.average_reward = 0\n",
    "        self.expected_reward = expected_reward\n",
    "        self.gradient = gradient\n",
    "        self.gradient_baseline = baseline\n",
    "        \n",
    "    def reset(self):\n",
    "        self.q_expected = np.random.randn(self.arm) + self.expected_reward\n",
    "        self.q_estimated = np.zeros(self.arm) + self.initial_value\n",
    "        self.action_count = np.zeros(self.arm)\n",
    "        self.time = 0\n",
    "        self.average_reward = 0\n",
    "        self.best_action = np.argmax(self.q_expected)\n",
    "        #print(self.q_expected)\n",
    "        #print(self.q_estimated)\n",
    "        \n",
    "    def selectAction(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.arm)\n",
    "        \n",
    "        if self.UCB_val is not None:\n",
    "            return np.argmax(self.q_estimated + self.UCB_val * np.sqrt(np.log(self.time + 1) / (self.action_count+1e-10)))\n",
    "        if self.gradient:\n",
    "            preference = np.exp(self.q_estimated)\n",
    "            self.action_probability = preference / np.sum(preference)\n",
    "            return np.random.choice(self.arm, p=self.action_probability)\n",
    "        \n",
    "        return np.argmax(self.q_estimated)\n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        self.time += 1\n",
    "        self.action_count[action] += 1\n",
    "        reward = np.random.randn() + self.q_expected[action]\n",
    "        #print(reward)\n",
    "        self.average_reward += (reward-self.average_reward)/self.time\n",
    "        #print((reward - self.q_estimated[action])/self.action_count[action])\n",
    "        if self.sample_average:\n",
    "            self.q_estimated[action] += (reward - self.q_estimated[action])/self.action_count[action]\n",
    "        elif self.gradient:\n",
    "            one_hot = np.zeros(self.arm)\n",
    "            one_hot[action] = 1\n",
    "            if self.gradient_baseline:\n",
    "                baseline = self.average_reward\n",
    "            else:\n",
    "                baseline = 0\n",
    "            self.q_estimated += self.step_size * (reward - baseline) * (one_hot - self.action_probability)\n",
    "        else:\n",
    "            self.q_estimated[action] += self.step_size * (reward - self.q_estimated[action])\n",
    "        return reward\n",
    "\n",
    "\n",
    "def run_experiment(runs, time_steps, bandits):\n",
    "    rewards = np.zeros((len(bandits), runs, time_steps))\n",
    "    #print(rewards)\n",
    "    print('Running experiment...')\n",
    "    #print(rewards.shape)\n",
    "    best_action_counts = np.zeros(rewards.shape)\n",
    "    for i, bandit in enumerate(bandits):\n",
    "        for r in trange(runs):\n",
    "            bandit.reset()\n",
    "            for t in range(time_steps):\n",
    "                action = bandit.selectAction()\n",
    "                reward = bandit.takeAction(action)\n",
    "                rewards[i, r, t] = reward\n",
    "                if action == bandit.best_action:\n",
    "                    best_action_counts[i, r, t] = 1\n",
    "    mean_best_action_counts = best_action_counts.mean(axis=1)\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    return mean_best_action_counts, mean_rewards\n",
    "\n",
    "def generateEpsGreedyFigure(runs=2000, time_steps=1000):\n",
    "    epsilons = [0, 0.1, 0.01]\n",
    "    bandits = [Bandit(eps=eps, sample_average = True) for eps in epsilons]\n",
    "    best_action_counts, rewards = run_experiment(runs, time_steps, bandits)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for eps, rewards in zip(epsilons, rewards):\n",
    "        plt.plot(rewards, label='epsilon = %.02f' % (eps))\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('average reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for eps, counts in zip(epsilons, best_action_counts):\n",
    "        plt.plot(counts, label='epsilon = %.02f' % (eps))\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('G:/rubel/HDILab/figure_2_2.png')\n",
    "    plt.close()\n",
    "    print('Generated Eps greedy..')\n",
    "    \n",
    "def generateOptimisticInitialValueFigure(runs=2000, time_steps=1000):\n",
    "    bandits = [Bandit(eps=0, init_value=5, step_size=0.1), \\\n",
    "               Bandit(eps=0.1, init_value=0, step_size=0.1)]\n",
    "    best_action_counts, _ = run_experiment(runs, time_steps, bandits)\n",
    "\n",
    "    plt.plot(best_action_counts[0], label='epsilon = 0, q = 5')\n",
    "    plt.plot(best_action_counts[1], label='epsilon = 0.1, q = 0')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('G:/rubel/HDILab/figure_2_3.png')\n",
    "    plt.close()\n",
    "    print('Generated optimistic initial..')\n",
    "\n",
    "def generateUCBFigure(runs=2000, time_steps=1000):\n",
    "    bandits = [Bandit(eps=0, UCB_val=2, sample_average=True), \\\n",
    "               Bandit(eps=0.1, sample_average=True)]\n",
    "    _, avg_rewards = run_experiment(runs, time_steps, bandits)\n",
    "\n",
    "    plt.plot(avg_rewards[0], label='UCB c = 2')\n",
    "    plt.plot(avg_rewards[1], label='eps greedy, eps = 0.1')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('G:/rubel/HDILab/figure_2_4.png')\n",
    "    plt.close()\n",
    "\n",
    "def generateGradientFigure(runs=2000, time_step=1000):\n",
    "    bandits = [Bandit(gradient=True, step_size=0.1, baseline=True, expected_reward=4), \\\n",
    "               Bandit(gradient=True, step_size=0.1, baseline=False, expected_reward=4),\\\n",
    "               Bandit(gradient=True, step_size=0.4, baseline=True, expected_reward=4), \\\n",
    "               Bandit(gradient=True, step_size=0.4, baseline=False, expected_reward=4)]\n",
    "\n",
    "    best_action_counts, _ = run_experiment(runs, time_step, bandits)\n",
    "    labels = ['alpha = 0.1, with baseline',\n",
    "              'alpha = 0.1, without baseline',\n",
    "              'alpha = 0.4, with baseline',\n",
    "              'alpha = 0.4, without baseline']\n",
    "\n",
    "    for i in range(len(bandits)):\n",
    "        plt.plot(best_action_counts[i], label=labels[i])\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% Optimal action')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('G:/rubel/HDILab/figure_2_5.png')\n",
    "    plt.close()\n",
    "\n",
    "def generateSummaryFigure(runs=2000, time_steps=1000):\n",
    "    labels = ['epsilon-greedy', 'gradient bandit',\n",
    "              'UCB', 'optimistic initialization']\n",
    "    generators = [lambda eps: Bandit(eps=eps, sample_average=True),\n",
    "                  lambda alpha: Bandit(gradient=True, step_size=alpha, baseline=True),\n",
    "                  lambda coef: Bandit(eps=0, UCB_val=coef, sample_average=True),\n",
    "                  lambda initial: Bandit(eps=0, init_value=initial, step_size=0.1)]\n",
    "    parameters = [np.arange(-7, -1, dtype=np.float),\n",
    "                  np.arange(-5, 2, dtype=np.float),\n",
    "                  np.arange(-4, 3, dtype=np.float),\n",
    "                  np.arange(-2, 3, dtype=np.float)]\n",
    "\n",
    "    bandits = []\n",
    "    for generator, parameter in zip(generators, parameters):\n",
    "        for param in parameter:\n",
    "            bandits.append(generator(pow(2, param)))\n",
    "\n",
    "    _, average_rewards = run_experiment(runs, time_steps, bandits)\n",
    "    rewards = np.mean(average_rewards, axis=1)\n",
    "\n",
    "    i = 0\n",
    "    for label, parameter in zip(labels, parameters):\n",
    "        l = len(parameter)\n",
    "        plt.plot(parameter, rewards[i:i+l], label=label)\n",
    "        i += l\n",
    "    plt.xlabel('Parameter(2^x)')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('G:/rubel/HDILab/figure_2_6.png')\n",
    "    plt.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #bandit = Bandit()\n",
    "    #bandit.reset()\n",
    "    #generateEpsGreedyFigure()\n",
    "    #generateOptimisticInitialValueFigure()\n",
    "    #generateUCBFigure()\n",
    "    #generateGradientFigure()\n",
    "    generateSummaryFigure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
